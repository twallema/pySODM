

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Modeling and Calibration Workflow &mdash; pySODM  documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/documentation_options.js?v=5929fcd5"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="A model for the enzymatic esterification of D-glucose and Lauric acid in a continuous-flow reactor" href="enzyme_kinetics.html" />
    <link rel="prev" title="Quickstart" href="quickstart.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            pySODM
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Modeling and calibration workflow</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#import-dependencies">Import dependencies</a></li>
<li class="toctree-l2"><a class="reference internal" href="#load-the-dataset">Load the dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="#define-and-initialize-the-model">Define and initialize the model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#calibrating-the-model">Calibrating the model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#the-posterior-probability-function">The posterior probability function</a></li>
<li class="toctree-l3"><a class="reference internal" href="#choosing-an-appropriate-prior-function">Choosing an appropriate prior function</a></li>
<li class="toctree-l3"><a class="reference internal" href="#choosing-an-appropriate-likelihood-function">Choosing an appropriate likelihood function</a></li>
<li class="toctree-l3"><a class="reference internal" href="#setting-up-the-posterior-probability-in-pysodm">Setting up the posterior probability in pySODM</a></li>
<li class="toctree-l3"><a class="reference internal" href="#nelder-mead-optimization">Nelder-Mead optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bayesian-inference">Bayesian inference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#results">Results</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#basic-reproduction-number">Basic reproduction number</a></li>
<li class="toctree-l3"><a class="reference internal" href="#goodness-of-fit-draw-functions">Goodness-of-fit: draw functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#scenarios-time-dependent-model-parameters">Scenarios: time-dependent model parameters</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Case studies</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="enzyme_kinetics.html">Enzyme kinetics</a></li>
<li class="toctree-l1"><a class="reference internal" href="influenza_1718.html">Influenza 2017-2018</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">Additional references</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="speedup.html">Speeding up models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="start_to_collaborate.html">Start to collaborate</a></li>
<li class="toctree-l1"><a class="reference internal" href="guidelines.html">Contribution guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="git_workflow.html">Git workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">pySODM</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Modeling and Calibration Workflow</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/workflow.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="modeling-and-calibration-workflow">
<h1>Modeling and Calibration Workflow<a class="headerlink" href="#modeling-and-calibration-workflow" title="Link to this heading">¶</a></h1>
<p>In this tutorial, we’ll use the simple SIR disease model without dimensions from the <a class="reference internal" href="quickstart.html"><span class="std std-doc">quickstart</span></a> tutorial and calibrate its basic reproduction number to a synthetically generated dataset. We’ll then asses what happens if the pathogen’s infectivity is lowered by means of preventive measures. This tutorial steps you through a typical workflow constituting the following steps,</p>
<ol class="arabic simple">
<li><p>Import dependencies</p></li>
<li><p>Load the dataset</p></li>
<li><p>Define a pySODM model</p></li>
<li><p>Initialize the model</p></li>
<li><p>Calibrate the model (PSO/NM + MCMC)</p></li>
<li><p>Visualize the goodness-of-fit</p></li>
<li><p>Perform a scenario analysis</p></li>
</ol>
<p>By using a simple model, we can focus on the general workflow and on the most important functions of pySODM, which will be similar in the more advanced <a class="reference internal" href="enzyme_kinetics.html"><span class="std std-doc">enzyme kinetics</span></a> and <a class="reference internal" href="influenza_1718.html"><span class="std std-doc">Influenza</span></a> case studies available on this documentation website. This tutorial can be reproduced by running <code class="docutils literal notranslate"><span class="pre">~/tutorials/SIR/workflow_tutorial.py</span></code>, an environment containing the dependencies needed to run this tutorial is provided in <code class="docutils literal notranslate"><span class="pre">~/tutorial_env.yml</span></code>.</p>
<section id="import-dependencies">
<h2>Import dependencies<a class="headerlink" href="#import-dependencies" title="Link to this heading">¶</a></h2>
<p>I typically place all my dependencies together at the top of my script. However, for this demo, we’ll import some common dependencies here and then import the pySODM code on the go. That way, the imports of the necessary pySODM code are located where they are required which is more illustrative than including them all here at once.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
</pre></div>
</div>
</section>
<section id="load-the-dataset">
<h2>Load the dataset<a class="headerlink" href="#load-the-dataset" title="Link to this heading">¶</a></h2>
<p>For the purpose of this tutorial, we’ll generate a sythetic dataset containing disease incidence. We assume the disease is generating cases exponentially with a doubling time of 10 days. Mathematically,</p>
<div class="math notranslate nohighlight">
\[n_{cases}(t) = \exp \Big( t *  \dfrac{\log 2}{t_d} \Big)\]</div>
<p>We’ll assume the first case was detected on December 1st, 2022 and data was collected on weekdays only until December 21st, 2022. Then, we’ll add observational noise to the synthetic data. For count based data, observational noise is typically the result of a poisson or negative binomial proces, depending on the occurence of overdispersion. In case the observations result from a poisson proces, the variance in the data is equal to the mean: <span class="math notranslate nohighlight">\(\sigma^2 = \mu\)</span>, while for a negative binomial proces the mean-variance relationship is quadratic: <span class="math notranslate nohighlight">\(\sigma^2 = \mu + \alpha \mu^2\)</span>. For this example we’ll use the negative binomial distribution with a dispersion factor of <code class="docutils literal notranslate"><span class="pre">alpha=0.03</span></code>, which was representative for the data used during the COVID-19 pandemic in Belgium. Note that for <span class="math notranslate nohighlight">\(\alpha=0\)</span>, the variance of the negative binomial distribution is equal to the variance of the poisson distribution.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Parameters </span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.03</span>    <span class="c1"># Overdispersion</span>
<span class="n">t_d</span> <span class="o">=</span> <span class="mi">10</span>         <span class="c1"># Exponential doubling time</span>
<span class="c1"># Sample data</span>
<span class="n">dates</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">date_range</span><span class="p">(</span><span class="s1">&#39;2022-12-01&#39;</span><span class="p">,</span><span class="s1">&#39;2023-01-21&#39;</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">dates</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">dates</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">negative_binomial</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">alpha</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">alpha</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">t</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">t_d</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">alpha</span><span class="p">)))</span>
<span class="c1"># Place in a pd.Series</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">dates</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;CASES&#39;</span><span class="p">)</span>
<span class="c1"># Index name must be date for calibration to work</span>
<span class="n">d</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;date&#39;</span>
<span class="c1"># Only retain weekdays</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">d</span><span class="p">[</span><span class="n">d</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">dayofweek</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
<p>Datasets used in an optimization must always be a <code class="docutils literal notranslate"><span class="pre">pd.Series</span></code>, in case the model has no dimensions, or a <code class="docutils literal notranslate"><span class="pre">pd.DataFrame</span></code> with a <code class="docutils literal notranslate"><span class="pre">pd.MultiIndex</span></code> if the model has dimensions. In the dataset, an index level named <code class="docutils literal notranslate"><span class="pre">time</span></code> (if the time axis consists of int/float) or <code class="docutils literal notranslate"><span class="pre">date</span></code> (if the time axis consists of dates) must always be present. In this tutorial, we’ll use dates and thus we rename the index of our dataset <code class="docutils literal notranslate"><span class="pre">date</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Index name must be data for calibration to work</span>
<span class="n">d</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;date&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>date
<span class="m">2022</span>-12-01<span class="w">     </span><span class="m">1</span>
<span class="m">2022</span>-12-02<span class="w">     </span><span class="m">1</span>
...
<span class="m">2023</span>-01-19<span class="w">    </span><span class="m">34</span>
<span class="m">2023</span>-01-20<span class="w">    </span><span class="m">24</span>
Name:<span class="w"> </span>CASES,<span class="w"> </span>dtype:<span class="w"> </span>int64
</pre></div>
</div>
<p>Visually,</p>
<p><img alt="synethetic_dataset" src="_images/synthetic_dataset.png" /></p>
</section>
<section id="define-and-initialize-the-model">
<h2>Define and initialize the model<a class="headerlink" href="#define-and-initialize-the-model" title="Link to this heading">¶</a></h2>
<p>As an example, we’ll set up a simple Susceptible-Infectious-Removed (SIR) disease model, schematically represented as follows,</p>
<img src="./_static/figs/quickstart/quickstart_SIR_flowchart.png" width="500" />
<p>and governed by the following equations,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}
N &amp;=&amp; S + I + R, \\
\frac{dS}{dt} &amp;=&amp; - \beta S (I/N), \\
\frac{dI}{dt} &amp;=&amp; \beta S (I/N) - (1/\gamma)I, \\
\frac{dR}{dt} &amp;=&amp; (1/\gamma)I.
\end{eqnarray}\end{split}\]</div>
<p>The model has three states: 1) The number of individuals susceptible to the disease (S), 2) the number of infectious individuals (I), and 3) the number of removed individuals (R). The model has two parameters: 1) <code class="docutils literal notranslate"><span class="pre">beta</span></code>, the rate of transmission and, 2) <code class="docutils literal notranslate"><span class="pre">gamma</span></code>, the duration of infectiousness. Building a pySODM model is based on class inheritance, you load the <code class="docutils literal notranslate"><span class="pre">ODE</span></code> class from <code class="docutils literal notranslate"><span class="pre">~/src/models/base.by</span></code>. Then, you define your own model class which must contain (minimally),</p>
<ul class="simple">
<li><p>A list containing the state names, named <code class="docutils literal notranslate"><span class="pre">states</span></code>,</p></li>
<li><p>A list containing the parameter names, named <code class="docutils literal notranslate"><span class="pre">parameter_names</span></code>,</p></li>
<li><p>A function named <code class="docutils literal notranslate"><span class="pre">integrate()</span></code> where you compute your model’s differentials.</p></li>
</ul>
<p>This class takes pySODM’s <code class="docutils literal notranslate"><span class="pre">ODE</span></code> class as its input. The <code class="docutils literal notranslate"><span class="pre">ODE</span></code> class is recognizes the components of your custom model class which will aid in performing input checks. Checkout the documentation of the <a class="reference internal" href="models.html"><span class="std std-doc">ODE class</span></a> for an overview of the class methods. There are some important formatting requirements to the integrate function, which are verified when the model is initialized,</p>
<ol class="arabic simple">
<li><p>The integrate function must have the timestep <code class="docutils literal notranslate"><span class="pre">t</span></code> as its first input</p></li>
<li><p>Followed by all model states and parameters, their order does not matter</p></li>
<li><p>The integrate function must return a differential for every model state, arranged in the same order as the state names defined in <code class="docutils literal notranslate"><span class="pre">states</span></code></p></li>
<li><p>The integrate function must be a static method (done by decorating with <code class="docutils literal notranslate"><span class="pre">&#64;staticmethod</span></code>)</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the ODE class</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pySODM.models.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">ODE</span>

<span class="c1"># Define the model equations</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ODE_SIR</span><span class="p">(</span><span class="n">ODE</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An SIR model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">states</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;S&#39;</span><span class="p">,</span><span class="s1">&#39;I&#39;</span><span class="p">,</span><span class="s1">&#39;R&#39;</span><span class="p">]</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span><span class="s1">&#39;gamma&#39;</span><span class="p">]</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">integrate</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
        
        <span class="c1"># Calculate total population</span>
        <span class="n">N</span> <span class="o">=</span> <span class="n">S</span><span class="o">+</span><span class="n">I</span><span class="o">+</span><span class="n">R</span>
        <span class="c1"># Calculate differentials</span>
        <span class="n">dS</span> <span class="o">=</span> <span class="o">-</span><span class="n">beta</span><span class="o">*</span><span class="n">S</span><span class="o">*</span><span class="n">I</span><span class="o">/</span><span class="n">N</span>
        <span class="n">dI</span> <span class="o">=</span> <span class="n">beta</span><span class="o">*</span><span class="n">S</span><span class="o">*</span><span class="n">I</span><span class="o">/</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="o">/</span><span class="n">gamma</span><span class="o">*</span><span class="n">I</span>
        <span class="n">dR</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">gamma</span><span class="o">*</span><span class="n">I</span>

        <span class="k">return</span> <span class="n">dS</span><span class="p">,</span> <span class="n">dI</span><span class="p">,</span> <span class="n">dR</span>
</pre></div>
</div>
<p>After defining our model, we’ll initialize it by supplying a dictionary of initial states and a dictionary of model parameters. In this example, we’ll assume the disease spreads in a relatively small population of 1000 individuals. At the start of the simulation we’ll assume there is one “patient zero”. There’s no need to define the number of recovered individuals as undefined states are automatically set to zero by pySODM.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">ODE_SIR</span><span class="p">(</span><span class="n">initial_states</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;S&#39;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span> <span class="s1">&#39;I&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span> <span class="n">parameters</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="mf">0.35</span><span class="p">,</span> <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">})</span>
</pre></div>
</div>
</section>
<section id="calibrating-the-model">
<h2>Calibrating the model<a class="headerlink" href="#calibrating-the-model" title="Link to this heading">¶</a></h2>
<section id="the-posterior-probability-function">
<h3>The posterior probability function<a class="headerlink" href="#the-posterior-probability-function" title="Link to this heading">¶</a></h3>
<p>Before we can have our computer find a set of model parameters that aligns the model with the data, we must instruct it what deviations between the data and model prediction are tolerated. Such function is often referred to as an <em>objective function</em> or <em>likelihood function</em>. In all tutorials we will set up and attempt to maximize the <em>posterior probability</em> of our model’s simulations in light of the data <span class="math notranslate nohighlight">\(p(\tilde{x}(\theta) | x)\)</span>, which is mathematically defined as,
$$ p (\tilde{x}(\theta) | x) = \frac{p( x | \tilde{x}(\theta)) p(\theta)}{p(x)}, $$
where <span class="math notranslate nohighlight">\(x\)</span> represents the data, <span class="math notranslate nohighlight">\(\theta\)</span> the model’s parameters, and <span class="math notranslate nohighlight">\(\tilde{x}(\theta)\)</span> a model simulation made using parameters <span class="math notranslate nohighlight">\(\theta\)</span>. <span class="math notranslate nohighlight">\(p(x | \tilde{x}(\theta))\)</span> is the <em>likelihood function</em>, <span class="math notranslate nohighlight">\(p(\theta)\)</span> is the <em>prior probability</em> of the model parameters and contains any prior beliefs about the probability density distribution of the parameters <span class="math notranslate nohighlight">\(\theta\)</span>. Finally, <span class="math notranslate nohighlight">\(p(x)\)</span> is the probability of the data, which is used for normalization and can be ignored for all practical purposes. What is therefore important to remember is that the <em>posterior probability</em> is proportional to the product of the <em>likelihood</em>  and the parameter <em>prior probability</em>. We’ll maximize the logarithm of the <em>posterior probability</em>, which can be computed as the sum of the logarithm of the <em>prior probability</em> and the logarithm of the <em>likelihood</em>. Therefore,</p>
<p>$$\log p (\tilde{x}(\theta) | x) \propto \log p( x | \tilde{x}(\theta)) + \log p(\theta)$$</p>
<p>Just remember: LOG POSTERIOR = LOG PRIOR + LOG LIKELIHOOD</p>
<p>For an introduction to Bayesian inference, I recommend reading the following <a class="reference external" href="https://towardsdatascience.com/a-gentle-introduction-to-bayesian-inference-6a7552e313cb">article</a>. I also recommend going through the following tutorial of <a class="reference external" href="https://emcee.readthedocs.io/en/stable/tutorials/line/">emcee</a>.</p>
</section>
<section id="choosing-an-appropriate-prior-function">
<h3>Choosing an appropriate prior function<a class="headerlink" href="#choosing-an-appropriate-prior-function" title="Link to this heading">¶</a></h3>
<p>For every model parameter you calibrate, a probability function expressing our prior believes with regard to the probability distribution of that parameter must be provided. pySODM includes uniform, triangular, normal, gamma and beta priors, which can be imported as follows,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pySODM.optimization.objective_functions</span><span class="w"> </span><span class="kn">import</span> <span class="n">log_prior_uniform</span><span class="p">,</span> <span class="n">log_prior_triangle</span><span class="p">,</span> <span class="n">log_prior_normal</span><span class="p">,</span> <span class="n">log_prior_gamma</span><span class="p">,</span> <span class="n">log_prior_beta</span>
</pre></div>
</div>
<p>When initializing the <a class="reference internal" href="optimization.html"><span class="std std-doc"><code class="docutils literal notranslate"><span class="pre">log_posterior</span> <span class="pre">probability</span></code> class</span></a> it is not necessary to define priors for your parameters (arguments <code class="docutils literal notranslate"><span class="pre">log_prior_prob_fnc</span></code> and <code class="docutils literal notranslate"><span class="pre">log_prior_prob_fnc_args</span></code>). If no priors are defined, by default, pySODM will initialize uniform priors for the calibrated parameters. For most problems, uniform prior probabilities – which simply constraint the parameter values within certain bounds – suffice.</p>
</section>
<section id="choosing-an-appropriate-likelihood-function">
<h3>Choosing an appropriate likelihood function<a class="headerlink" href="#choosing-an-appropriate-likelihood-function" title="Link to this heading">¶</a></h3>
<p>The next step is to choose an appropriate log likelihood function. The log likelihood function is a function that describes the magnitude of the error when model prediction and data deviate. The bread and butter log likelihood function is the sum of squared errors (SSE),</p>
<div class="math notranslate nohighlight">
\[SSE = \sum_i (y_{data,i} - y_{model,i})^2,\]</div>
<p>which is a simplified case of a normal log likelihood function,</p>
<div class="math notranslate nohighlight">
\[\log \big[ p(y_{data} | y_{model}, \sigma) \big] = - \frac{1}{2} \sum_i \Bigg[ \frac{(y_{data,i} - y_{model,i})^2}{\sigma_i^2} + \log (2 \pi \sigma_i^2) \Bigg].\]</div>
<p>The normal log likelihood reduces to the SSE when the error on all datapoints are the same (i.e. <span class="math notranslate nohighlight">\(\sigma_i^2 = 1\)</span>), in technical terms, the SSE assumes the data are homoskedastic which is rarely the case in practice. If the errors (<span class="math notranslate nohighlight">\(\sigma_i\)</span>) of all datapoints (<span class="math notranslate nohighlight">\(y_{data,i}\)</span>) are known, then the normal log likelihood function is the maximum likelihood estimator. When the error of the datapoints are unknown, we can analyze the relationship between the magnitude of the data and its variance to help us choose the most appropriate likelihood function. For epidemiological case data, only one datapoint is available per day implying no error is readily available. However, in count data dispersion tends to increase linearily with the magnitude of the data. In that case, pySODM’s <a class="reference internal" href="optimization.html"><span class="std std-doc"><code class="docutils literal notranslate"><span class="pre">variance_analysis()</span></code> function</span></a> includes a functionality to approximate the mean-variance relationship in a dataset of counts. To do so, we assume a moving exponential average of the data accurately represents the underlying undispersed truth, we then dividing the data in discrete windows, and then compute the dispersion of the data around the exponential moving average’s mean within every window to obtain mean-variance couples. Then, the appropriate likelihood function can be found by fitting the following candidate mean-variance relationships,</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Mean-Variance model</p></th>
<th class="head"><p>Relationship</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Gaussian</p></td>
<td><p><span class="math notranslate nohighlight">\(\sigma^2 = c\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Poisson</p></td>
<td><p><span class="math notranslate nohighlight">\(\sigma^2 = \mu\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Quasi-Poisson</p></td>
<td><p><span class="math notranslate nohighlight">\(\sigma^2 = \alpha * \mu\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Negative Binomial</p></td>
<td><p><span class="math notranslate nohighlight">\(\sigma^2 = \mu + \alpha * \mu^2\)</span></p></td>
</tr>
</tbody>
</table>
<p>For a more rigorous explanation of the procedure, we highly recommend reading section E.1 in the Supplementary Materials of <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0307904X23002810">this article</a>. The following code snippet performs the estimation for our synthetic dataset.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pySODM.optimization.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">variance_analysis</span>

<span class="n">results</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">variance_analysis</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">window_length</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">half_life</span><span class="o">=</span><span class="mf">3.5</span><span class="p">)</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;negative binomial&#39;</span><span class="p">,</span> <span class="s1">&#39;theta&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">                       </span>theta<span class="w">        </span>AIC
gaussian<span class="w">           </span><span class="m">14</span>.568640<span class="w">  </span><span class="m">52</span>.300657
poisson<span class="w">             </span><span class="m">0</span>.000000<span class="w">  </span><span class="m">44</span>.165044
quasi-poisson<span class="w">       </span><span class="m">1</span>.925682<span class="w">  </span><span class="m">38</span>.796586
negative<span class="w"> </span>binomial<span class="w">   </span><span class="m">0</span>.042804<span class="w">  </span><span class="m">32</span>.045864
</pre></div>
</div>
<p>The negative binomial model with dispersion coefficient <span class="math notranslate nohighlight">\(\alpha = 0.043\)</span> is the most appropriate statistical model (lowest AIC). This estimate is quite good considering we’re using a very limited amount of data generated from a negative binomial model with a dispersion coefficient <span class="math notranslate nohighlight">\(\alpha = 0.03\)</span>. We highly recommend varying both the window size and the length of the exponential filter before committing to any likelihood function.</p>
<p><img alt="variance_analysis" src="_images/variance_analysis.png" /></p>
</section>
<section id="setting-up-the-posterior-probability-in-pysodm">
<h3>Setting up the posterior probability in pySODM<a class="headerlink" href="#setting-up-the-posterior-probability-in-pysodm" title="Link to this heading">¶</a></h3>
<p>Let’s initialize an appropriate <a class="reference internal" href="optimization.html"><span class="std std-doc">posterior probability function</span></a> for the problem at hand. We start by importing the <code class="docutils literal notranslate"><span class="pre">log_posterior_probability</span></code> class and the negative binomial likelihood function <code class="docutils literal notranslate"><span class="pre">ll_negative_binomial</span></code>. The following arguments of <code class="docutils literal notranslate"><span class="pre">log_posterior_probability</span></code> are mandatory,</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model</span></code>: The previously initialized model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pars</span></code>: A list containing the names of the model parameters we wish to optimize. pySODM can be used to calibrate 0D, 1D and nD model parameters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bounds</span></code>: A list containing the lower and an upper bounds of the parameters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data</span></code>: A list containing the datasets we wish to calibrate our model to.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">states</span></code>: A list containing, for every dataset, the model state that must be matched with it.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">log_likelihood_function</span></code>: A list containing, for every dataset, the log likelihood function used to describe deviations between the model prediction and the respective dataset.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">log_likelihood_function_args</span></code>: A list containing the arguments of every log likelihood function.</p></li>
</ol>
<p>The lengths of the number of <code class="docutils literal notranslate"><span class="pre">data</span></code>, <code class="docutils literal notranslate"><span class="pre">states</span></code>, <code class="docutils literal notranslate"><span class="pre">log_likelihood</span> <span class="pre">function</span></code> and <code class="docutils literal notranslate"><span class="pre">log_likelihood_function_args</span></code> must always be equal. In the example, as no prior functions are provided, the priors will default to uniform priors over the provided bounds. Providing a label is also optional, if no label is provided, the names provided in <code class="docutils literal notranslate"><span class="pre">pars</span></code> are used as labels.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    
    <span class="c1"># Import the log_posterior_probability class</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">pySODM.optimization.objective_functions</span><span class="w"> </span><span class="kn">import</span> <span class="n">log_posterior_probability</span>

    <span class="c1"># Import the negative binomial likelihood function</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">pySODM.optimization.objective_functions</span><span class="w"> </span><span class="kn">import</span> <span class="n">ll_negative_binomial</span>

    <span class="c1"># The datasets, the model states to match to the datasets,</span>
    <span class="c1"># the log likelihood functions and log likelihood function arguments</span>
    <span class="n">data</span><span class="o">=</span><span class="p">[</span><span class="n">d</span><span class="p">,</span> <span class="p">]</span>
    <span class="n">states</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;I&quot;</span><span class="p">,]</span>
    <span class="n">log_likelihood_fnc</span> <span class="o">=</span> <span class="p">[</span><span class="n">ll_negative_binomial</span><span class="p">,]</span>
    <span class="n">log_likelihood_fnc_args</span> <span class="o">=</span> <span class="p">[</span><span class="n">alpha</span><span class="p">,]</span>

    <span class="c1"># Calibated parameters, their bounds and preferred labels</span>
    <span class="n">pars</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">,]</span>
    <span class="n">bounds</span> <span class="o">=</span> <span class="p">[(</span><span class="mf">1e-6</span><span class="p">,</span><span class="mi">1</span><span class="p">),]</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">beta$&#39;</span><span class="p">,]</span>
    
    <span class="c1"># Setup objective function</span>
    <span class="c1"># (no priors --&gt; uniform priors based on bounds)</span>
    <span class="n">objective_function</span> <span class="o">=</span> <span class="n">log_posterior_probability</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pars</span><span class="p">,</span> <span class="n">bounds</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">log_likelihood_fnc</span><span class="p">,</span> <span class="n">log_likelihood_fnc_args</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
<p>If we wanted to use a normal prior distribution for <code class="docutils literal notranslate"><span class="pre">beta</span></code>,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the normal prior probability distribution</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pySODM.optimization.objective_functions</span><span class="w"> </span><span class="kn">import</span> <span class="n">log_prior_normal</span>

<span class="c1"># Define the additional arguments of the log_posterior_probability class</span>
<span class="n">log_prior_prob_fnc</span> <span class="o">=</span> <span class="p">[</span><span class="n">log_prior_normal</span><span class="p">,]</span>
<span class="n">log_prior_prob_func_args</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;avg&#39;</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span> <span class="s1">&#39;stdev&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">},]</span>

<span class="c1"># Setup objective function</span>
<span class="n">objective_function</span> <span class="o">=</span> <span class="n">log_posterior_probability</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pars</span><span class="p">,</span> <span class="n">bounds</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">log_likelihood_fnc</span><span class="p">,</span> <span class="n">log_likelihood_fnc_args</span><span class="p">,</span> <span class="n">log_prior_prob_fnc</span><span class="p">,</span> <span class="n">log_prior_prob_func_args</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="nelder-mead-optimization">
<h3>Nelder-Mead optimization<a class="headerlink" href="#nelder-mead-optimization" title="Link to this heading">¶</a></h3>
<p>The following code snippet starts a Nelder-Mead optimization from the initial guess <span class="math notranslate nohighlight">\(\beta = 0.35\)</span>, perturbated with a <code class="docutils literal notranslate"><span class="pre">step</span></code> of 10%. Running on <code class="docutils literal notranslate"><span class="pre">processes=1</span></code> cores for <code class="docutils literal notranslate"><span class="pre">max_iter=10</span></code> iterations.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c1"># Import the Nelder-Mead algorithm</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">pySODM.optimization</span><span class="w"> </span><span class="kn">import</span> <span class="n">nelder_mead</span>
    <span class="c1"># Initial guess</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.35</span><span class="p">,]</span>
    <span class="c1"># Run Nelder-Mead optimisation</span>
    <span class="n">theta</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">nelder_mead</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">objective_function</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="p">[</span><span class="mf">0.10</span><span class="p">,],</span> <span class="n">processes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>We find an optimal value of <span class="math notranslate nohighlight">\(\beta \pm 0.27\)</span>. We can then asses the goodness-of-fit by updating the dictionary of model parameters with the newly found value for <code class="docutils literal notranslate"><span class="pre">beta</span></code>, simulating the model and visualizing the model prediction and dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c1"># Update beta with the calibrated value</span>
    <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]})</span>
    <span class="c1"># Simulate the model</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sim</span><span class="p">([</span><span class="n">d</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">d</span><span class="p">,</span><span class="n">index</span><span class="o">.</span><span class="n">max</span><span class="p">()])</span>
    <span class="c1"># Visualize result</span>
    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">],</span> <span class="n">out</span><span class="p">[</span><span class="s1">&#39;I&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Infectious&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">d</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;None&#39;</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
<p>Quite nice!</p>
<p><img alt="nelder_mead" src="_images/nelder_mead_fit.png" /></p>
</section>
<section id="bayesian-inference">
<h3>Bayesian inference<a class="headerlink" href="#bayesian-inference" title="Link to this heading">¶</a></h3>
<p>This approach moves away from the idea of accepting a single parameter value being the best fit to the data, and instead identifies all regions of the parameter space that are in agreement with the observations. In the language of Bayesian inference, what we seek is called the posterior distribution of the parameters: a probability distribution on the parameter space that assigns higher probability to areas that are in better agreement with the observations. Here, we demonstrate that a Bayesian approach provides accurate estimates of model parameters and their uncertainty.</p>
<p>We’ll use our previous estimate to initiate our Markov-Chain Monte-Carlo sampler. This requires the help of two functions: <a class="reference internal" href="optimization.html"><span class="std std-doc">perturbate_theta()</span></a> and <a class="reference internal" href="optimization.html"><span class="std std-doc">run_EnsembleSampler()</span></a>. We’ll initiate 9 chains per calibrated parameter, so there will be 9 chains in total. To do so, we’ll first use <code class="docutils literal notranslate"><span class="pre">perturbate_theta</span></code> to perturbate our previously obtained estimate <code class="docutils literal notranslate"><span class="pre">theta</span></code> by 10%. The result is a np.ndarray <code class="docutils literal notranslate"><span class="pre">pos</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">9)</span></code>, which we’ll then pass on to <code class="docutils literal notranslate"><span class="pre">run_EnsembleSampler()</span></code>.</p>
<p>Then, we’ll setup and run the sampler using <code class="docutils literal notranslate"><span class="pre">run_EnsembleSampler()</span></code> until the chains converge. We’ll run the sampler for <code class="docutils literal notranslate"><span class="pre">n_mcmc=100</span></code> iterations and print the diagnostic autocorrelation and trace plots every <code class="docutils literal notranslate"><span class="pre">print_n=10</span></code> iterations in a folder called <code class="docutils literal notranslate"><span class="pre">sampler_output/</span></code>. For convenience, a copy of the samples is saved in an <code class="docutils literal notranslate"><span class="pre">xarray.Dataset</span></code> as well every <code class="docutils literal notranslate"><span class="pre">print_n=10</span></code> iterations. As an identifier for our <em>experiment</em>, we’ll use <code class="docutils literal notranslate"><span class="pre">'username'</span></code>. While the sampler is running, have a look in the <code class="docutils literal notranslate"><span class="pre">sampler_output/</span></code> folder, which should look as follows,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>├──<span class="w"> </span>sampler_output<span class="w"> </span>
<span class="p">|</span><span class="w">   </span><span class="p">|</span>──<span class="w"> </span>username_BACKEND_YYYY-MM-DD.hdf5
<span class="p">|</span><span class="w">   </span><span class="p">|</span>──<span class="w"> </span>username_SAMPLES_YYYY-MM-DD.nc
<span class="p">|</span><span class="w">   </span><span class="p">|</span>──<span class="w"> </span>username_AUTOCORR_YYYY-MM-DD.pdf
│<span class="w">   </span>└──<span class="w"> </span>username_TRACE_YYYY-MM-DD.pdf
</pre></div>
</div>
<p>The first output of <code class="docutils literal notranslate"><span class="pre">run_EnsembleSampler()</span></code> is an <code class="docutils literal notranslate"><span class="pre">emcee.EnsembleSampler</span></code> object containing our 100 iterations for 9 chains. We can extract the samples manually by using its <code class="docutils literal notranslate"><span class="pre">get_chain()</span></code> method, along with many more <code class="docutils literal notranslate"><span class="pre">emcee</span></code> related operations (see the <a class="reference external" href="https://emcee.readthedocs.io/en/stable/user/sampler/">emcee documentation</a>). However, we’re interested in the second output of <code class="docutils literal notranslate"><span class="pre">run_EnsembleSampler()</span></code>, which contains the samples in an <code class="docutils literal notranslate"><span class="pre">xarray.Dataset</span></code>, indexed on the chain and iteration numbers. This interfaces nicely to pySODM’s <em>draw functions</em> (introduced below). Finally, we’ll use the third-party <code class="docutils literal notranslate"><span class="pre">corner</span></code> package to visualize the distributions of the five calibrated parameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>

    <span class="kn">from</span><span class="w"> </span><span class="nn">pySODM.optimization.mcmc</span><span class="w"> </span><span class="kn">import</span> <span class="n">perturbate_theta</span><span class="p">,</span> <span class="n">run_EnsembleSampler</span>
    <span class="c1"># Settings</span>
    <span class="n">n_mcmc</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">multiplier_mcmc</span> <span class="o">=</span> <span class="mi">9</span>
    <span class="n">processes</span> <span class="o">=</span> <span class="mi">9</span>
    <span class="n">print_n</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">discard</span> <span class="o">=</span> <span class="mi">20</span>
    <span class="n">samples_path</span> <span class="o">=</span> <span class="s1">&#39;sampler_output/&#39;</span>
    <span class="n">fig_path</span> <span class="o">=</span> <span class="s1">&#39;sampler_output/&#39;</span>
    <span class="n">identifier</span> <span class="o">=</span> <span class="s1">&#39;username&#39;</span>
    <span class="c1"># Perturbate previously obtained estimate</span>
    <span class="n">ndim</span><span class="p">,</span> <span class="n">nwalkers</span><span class="p">,</span> <span class="n">pos</span> <span class="o">=</span> <span class="n">perturbate_theta</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">pert</span><span class="o">=</span><span class="p">[</span><span class="mf">0.10</span><span class="p">,],</span> <span class="n">multiplier</span><span class="o">=</span><span class="n">multiplier_mcmc</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">)</span>
    <span class="c1"># Usefull settings to retain in the samples dictionary (no pd.Timestamps or np.arrays allowed!)</span>
    <span class="n">settings</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;start_calibration&#39;</span><span class="p">:</span> <span class="n">d</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%Y-%m-</span><span class="si">%d</span><span class="s2">&quot;</span><span class="p">),</span> <span class="s1">&#39;end_calibration&#39;</span><span class="p">:</span> <span class="n">end_date</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%Y-%m-</span><span class="si">%d</span><span class="s2">&quot;</span><span class="p">),</span> <span class="s1">&#39;starting_estimate&#39;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="n">theta</span><span class="p">)}</span>
    <span class="c1"># Run the sampler</span>
    <span class="n">sampler</span><span class="p">,</span> <span class="n">samples_xr</span> <span class="o">=</span> <span class="n">run_EnsembleSampler</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">n_mcmc</span><span class="p">,</span> <span class="n">identifier</span><span class="p">,</span> <span class="n">objective_function</span><span class="p">,</span> <span class="n">fig_path</span><span class="o">=</span><span class="n">fig_path</span><span class="p">,</span> <span class="n">samples_path</span><span class="o">=</span><span class="n">samples_path</span><span class="p">,</span> <span class="n">print_n</span><span class="o">=</span><span class="n">print_n</span><span class="p">,</span> <span class="n">processes</span><span class="o">=</span><span class="n">processes</span><span class="p">,</span> <span class="n">progress</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">settings_dict</span><span class="o">=</span><span class="n">settings</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">samples_xr</span><span class="p">)</span>
</pre></div>
</div>
<p>The resulting <code class="docutils literal notranslate"><span class="pre">xarray.Dataset</span></code> contains the samples of <code class="docutils literal notranslate"><span class="pre">beta</span></code>, as well as the variables defined in the <code class="docutils literal notranslate"><span class="pre">settings</span></code> dictionary. It is identical to the samples automatically saved in <code class="docutils literal notranslate"><span class="pre">sampler_output/username_SAMPLES_YYYY-MM-DD.nc</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&lt;xarray.Dataset&gt;<span class="w"> </span>Size:<span class="w"> </span>24kB
Dimensions:<span class="w">    </span><span class="o">(</span>iteration:<span class="w"> </span><span class="m">300</span>,<span class="w"> </span>chain:<span class="w"> </span><span class="m">9</span><span class="o">)</span>
Coordinates:
<span class="w">  </span>*<span class="w"> </span>iteration<span class="w">  </span><span class="o">(</span>iteration<span class="o">)</span><span class="w"> </span>int64<span class="w"> </span>2kB<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">4</span><span class="w"> </span>...<span class="w"> </span><span class="m">296</span><span class="w"> </span><span class="m">297</span><span class="w"> </span><span class="m">298</span><span class="w"> </span><span class="m">299</span>
<span class="w">  </span>*<span class="w"> </span>chain<span class="w">      </span><span class="o">(</span>chain<span class="o">)</span><span class="w"> </span>int64<span class="w"> </span>72B<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">8</span>
Data<span class="w"> </span>variables:
<span class="w">    </span>beta<span class="w">       </span><span class="o">(</span>iteration,<span class="w"> </span>chain<span class="o">)</span><span class="w"> </span>float64<span class="w"> </span>22kB<span class="w"> </span><span class="m">0</span>.2818<span class="w"> </span>...<span class="w"> </span><span class="m">0</span>.2761
Attributes:
<span class="w">    </span>start_calibration:<span class="w">  </span><span class="m">2022</span>-12-01
<span class="w">    </span>end_calibration:<span class="w">    </span><span class="m">2023</span>-01-20
<span class="w">    </span>starting_estimate:<span class="w">  </span><span class="o">[</span><span class="m">0</span>.2764298804104327<span class="o">]</span>
</pre></div>
</div>
</section>
</section>
<section id="results">
<h2>Results<a class="headerlink" href="#results" title="Link to this heading">¶</a></h2>
<section id="basic-reproduction-number">
<h3>Basic reproduction number<a class="headerlink" href="#basic-reproduction-number" title="Link to this heading">¶</a></h3>
<p>For our simple SIR model, the basic reproduction number is defined as,</p>
<div class="math notranslate nohighlight">
\[R_0 = \frac{\beta}{(1/\gamma)}.\]</div>
<p>Using the obtained samples, we find a basic reproduction number of <span class="math notranslate nohighlight">\(R_0 = 1.37 \pm 0.01\)</span>. Its distribution looks as follows,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize the distribution of the basic reproduction number</span>
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples_xr</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">*</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">],</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$R_0$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="R0" src="_images/reproduction_number.png" /></p>
</section>
<section id="goodness-of-fit-draw-functions">
<h3>Goodness-of-fit: draw functions<a class="headerlink" href="#goodness-of-fit-draw-functions" title="Link to this heading">¶</a></h3>
<p>Next, let’s visualize how well our simple SIR model fits the data. To this end, we’ll simulate the model a number of times, and we’ll update the value of <code class="docutils literal notranslate"><span class="pre">beta</span></code> with a sample from its posterior probability distribution obtained using the MCMC. Then, we’ll add the noise introduced by observing the epidemiological process to each simulated model trajectory using <code class="docutils literal notranslate"><span class="pre">add_negative_binomial()</span></code>. Finally, we’ll visualize the individual model trajectories and the data to asses the goodness-of-fit.</p>
<p>To repeatedly simulate a model and update a parameter value in each consecutive run, you can use pySODM’s <em>draw function</em>. These functions <strong>always</strong> take the model <code class="docutils literal notranslate"><span class="pre">parameters</span></code> as its first input argument, followed by an arbitrary number of user-defined parameters to aid in the draw function. A draw function must always return the dictionary of model <code class="docutils literal notranslate"><span class="pre">parameters</span></code>, without alterations to the dictionaries keys. The draw function defines how parameters and initial conditions can change during consecutive simulations of the model, i.e. it updates parameter values in the model parameters dictionary. This feature is useful to perform sensitivty analysis.</p>
<p>In this example, we’ll use a draw function to replace <code class="docutils literal notranslate"><span class="pre">beta</span></code> with a random value obtained from its posterior probability distribution obtained during sampling. We accomplish this by defining a draw function with one additional argument, <code class="docutils literal notranslate"><span class="pre">samples</span></code>, which is a list containing the samples of the posterior probability distribution of <code class="docutils literal notranslate"><span class="pre">beta</span></code>. <code class="docutils literal notranslate"><span class="pre">np.random.choice()</span></code> is used to sample a random value of <code class="docutils literal notranslate"><span class="pre">beta</span></code> and assign it to the model parameteres dictionary,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define draw function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">draw_fcn</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">samples</span><span class="p">):</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
<p>To use this draw function, you provide four additional arguments to the <code class="docutils literal notranslate"><span class="pre">sim()</span></code> function,</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">N</span></code>: the number of repeated simulations,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">draw_function</span></code>: the draw function,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">draw_function_kwargs</span></code>: a dictionary containing all parameters of the draw function not equal to <code class="docutils literal notranslate"><span class="pre">parameters</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">processes</span></code>: the number of cores to divide the <code class="docutils literal notranslate"><span class="pre">N</span></code> simulations over.</p></li>
</ol>
<p>As demonstrated in the quickstart example, the <code class="docutils literal notranslate"><span class="pre">xarray.Dataset</span></code> containing the model output will now contain an additional dimension to accomodate the repeated simulations: <code class="docutils literal notranslate"><span class="pre">draws</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simulate model</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sim</span><span class="p">([</span><span class="n">d</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">d</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">+</span><span class="n">pd</span><span class="o">.</span><span class="n">Timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">28</span><span class="p">)],</span> <span class="n">N</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">draw_function</span><span class="o">=</span><span class="n">draw_fcn</span><span class="p">,</span> <span class="n">draw_function_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;samples&#39;</span><span class="p">:</span> <span class="n">samples_xr</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">flatten</span><span class="p">()},</span> <span class="n">processes</span><span class="o">=</span><span class="n">processes</span><span class="p">)</span>
<span class="c1"># Add negative binomial observation noise</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">add_negative_binomial_noise</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dispersion</span><span class="p">)</span>
<span class="c1"># Visualize result</span>
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">],</span> <span class="n">out</span><span class="p">[</span><span class="s1">&#39;I&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isel</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="n">i</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">],</span> <span class="n">out</span><span class="p">[</span><span class="s1">&#39;I&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="s1">&#39;draws&#39;</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">d</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;None&#39;</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">MaxNLocator</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
<p>Our simple SIR model fits the data well. From the projection we can deduce that the peak number of infectees will fall around February 7th, 2023 and there will be between 30-50 infectees.</p>
<p><img alt="MCMC" src="_images/MCMC_fit.png" /></p>
</section>
<section id="scenarios-time-dependent-model-parameters">
<h3>Scenarios: time-dependent model parameters<a class="headerlink" href="#scenarios-time-dependent-model-parameters" title="Link to this heading">¶</a></h3>
<p>Finally, let us introduce the concept of varying model parameters over the course of the simulation. We would like to model what would happen if we could somehow reduce the infectivity of the disease by 50% starting January 21st. In the real world this could be accomplished by raising awareness to the disease and applying preventive measures.</p>
<p>We’ll first need to implement a function to let our model know how a given parameter should vary over time: a <em>time-dependent model parameter function</em> (TDPF). Then, we’ll need to re-initialize our model and tell it what model parameter should be varied in accordance to our function. For our problem, a TDPF would have the following syntax,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a time-dependent parameter function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lower_infectivity</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="n">start_measures</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="n">start_measures</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">param</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.50</span><span class="o">*</span><span class="n">param</span>
</pre></div>
</div>
<p>This simple function reduces the parameter <code class="docutils literal notranslate"><span class="pre">param</span></code> with a factor two if the simulation date is larger than <code class="docutils literal notranslate"><span class="pre">start_measures</span></code>. A time-dependent model parameter function must always have the arguments <code class="docutils literal notranslate"><span class="pre">t</span></code> (simulation timestep), <code class="docutils literal notranslate"><span class="pre">states</span></code> (dictionary of model states) and <code class="docutils literal notranslate"><span class="pre">param</span></code> (original value of the parameter the function is applied to) as inputs. In addition, the user can supply additional arguments to the function, which need to be added to the dictionary of model parameters when the model is initialised.</p>
<p>We will apply this function to the infectivity parameter <code class="docutils literal notranslate"><span class="pre">beta</span></code>, and declare this using the <code class="docutils literal notranslate"><span class="pre">time_dependent_parameters</span></code> argument when initialising the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Attach the additional arguments of the time-depenent function to the parameter dictionary</span>
<span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;start_measures&#39;</span><span class="p">:</span> <span class="s1">&#39;2023-01-21&#39;</span><span class="p">})</span>

<span class="c1"># Initialize the model with the time dependent parameter funtion</span>
<span class="n">model_with</span> <span class="o">=</span> <span class="n">ODE_SIR</span><span class="p">(</span><span class="n">initial_states</span><span class="o">=</span><span class="n">init_states</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
                     <span class="n">time_dependent_parameters</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="n">lower_infectivity</span><span class="p">})</span>
</pre></div>
</div>
<p>Next, we compare the simulations with and without the use of our time-dependent model parameter function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simulate the model</span>
<span class="n">out_with</span> <span class="o">=</span> <span class="n">model_with</span><span class="o">.</span><span class="n">sim</span><span class="p">([</span><span class="n">d</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">d</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">+</span><span class="n">pd</span><span class="o">.</span><span class="n">Timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="mi">28</span><span class="p">)],</span> <span class="n">N</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">draw_function</span><span class="o">=</span><span class="n">draw_fcn</span><span class="p">,</span> <span class="n">draw_function_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;samples&#39;</span><span class="p">:</span> <span class="n">samples_xr</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">flatten</span><span class="p">()}</span> <span class="n">processes</span><span class="o">=</span><span class="n">processes</span><span class="p">)</span>

<span class="c1"># Add negative binomial observation noise</span>
<span class="n">out_with</span> <span class="o">=</span> <span class="n">add_negative_binomial_noise</span><span class="p">(</span><span class="n">out_with</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

<span class="c1"># Visualize result</span>
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">],</span> <span class="n">out</span><span class="p">[</span><span class="s1">&#39;I&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isel</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="n">i</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">out_with</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">],</span> <span class="n">out_with</span><span class="p">[</span><span class="s1">&#39;I&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isel</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="n">i</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">],</span> <span class="n">out</span><span class="p">[</span><span class="s1">&#39;I&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="s1">&#39;draws&#39;</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">out_with</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">],</span> <span class="n">out_with</span><span class="p">[</span><span class="s1">&#39;I&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="s1">&#39;draws&#39;</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">d</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;None&#39;</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">MaxNLocator</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Number of infected&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
<p>As we could reasonably expect, if we could implement some form of preventive measures on January 21st that slashes the infectivity in half, cases quite abruptly start declining and thus less people contrapt the disease.</p>
<p><img alt="scenario" src="_images/scenario.png" /></p>
<p>However, it is unlikely that people would adopt these preventive measures instantly. Adoptation of measures is more gradual in the real world. We could tackle this problem in two ways using pySODM,</p>
<ul class="simple">
<li><p>Implementing a ramp function to gradually lower the infectivity in our time-dependent model parameter function,</p></li>
<li><p>Using <em>draw functions</em> to sample <code class="docutils literal notranslate"><span class="pre">start_measures</span></code> stochastically in every simulation. We’ll demonstrate this option.</p></li>
</ul>
<p>To simulate ramp-like adoptation of measures, we can add the number of additional days it takes beyond January 21st, 2023 to adopt the measures by sampling from a triangular distribution with a minimum and mode of zero days, and a maximum adoptation time of <code class="docutils literal notranslate"><span class="pre">ramp_length</span></code> days. All we have to do is add one line to the draw function,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define draw function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">draw_fcn</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">samples</span><span class="p">,</span> <span class="n">ramp_length</span><span class="p">):</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;start_measures&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">triangular</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">mode</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">right</span><span class="o">=</span><span class="n">ramp_length</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
<p>Don’t forget to add the new parameter <code class="docutils literal notranslate"><span class="pre">ramp_length</span></code> to the dictionary of <code class="docutils literal notranslate"><span class="pre">draw_function_kwargs</span></code> when simulating the model! Gradually adopting the preventive measures results in a more realistic simulation,</p>
<p><img alt="scenario_gradual" src="_images/scenario_gradual.png" /></p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">¶</a></h2>
<p>I hope this tutorial has demonstrated a simple yet common workflow pySODM can speedup for you. However, both our model and dataset had no labeled dimensions (0-dimensional) so this example was very rudimentary. However, pySODM allows to tackle higher-dimensional problems using the same basic syntax. This allows to tackle complex problems in different scientific disciplines, I illustrate this with the following tutorials,</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Case study</p></th>
<th class="head"><p>Features</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Enzyme kinetics: intrinsic kinetics</p></td>
<td><p>Calibration of a model to multiple datasets with changing initial conditions.</p></td>
</tr>
<tr class="row-odd"><td><p>Enzyme kinetics: 1D Plug-Flow Reactor</p></td>
<td><p>Use the method of lines to discretise a PDE model into an ODE model and simulate it using pySODM.</p></td>
</tr>
<tr class="row-even"><td><p>Influenza 2017-2018</p></td>
<td><p>Stochastic jump process epidemiological model with age groups (1D model). Calibration of a 1D model parameter to a 1D dataset.</p></td>
</tr>
<tr class="row-odd"><td><p>SIR-SI Model</p></td>
<td><p>ODE model where states have different dimensions and thus different shapes.</p></td>
</tr>
</tbody>
</table>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="quickstart.html" class="btn btn-neutral float-left" title="Quickstart" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="enzyme_kinetics.html" class="btn btn-neutral float-right" title="A model for the enzymatic esterification of D-glucose and Lauric acid in a continuous-flow reactor" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Tijs Alleman.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>